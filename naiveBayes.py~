import nltk
import pickle

def get_words_in_tweets(tweets):
    all_words = []
    for (words, sentiment) in tweets:
    	all_words.extend(words)
    return all_words

def get_word_features(wordlist):
    wordlist = nltk.FreqDist(wordlist)
    word_features = wordlist.keys()
    return word_features

def extract_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['contains(%s)' % word] = (word in document_words)
    return features

fin = open('tweets.list',"r")
string = fin.read()
tweets = pickle.loads(string)
fin.close()

tweets_filtered = []

for tweet, tweet_type in tweets:
	words = tweet.split(" ")
	words_temp = []
	for e in words :
		word = ""
		if e.startswith("@") or e.startswith("http"):
			break;
		for i in range(len(e)):
			if e[i].isalpha() or e[i]=="'":
				word+=e[i]
			else :
				words_temp.append(word)
				word=""
	filtered = [e.lower() for e in words_temp if e not in nltk.corpus.stopwords.words('english')]
	tweets_filtered.append((filtered,tweet_type))

word_features = get_word_features(get_words_in_tweets(tweets_filtered))
training_set = nltk.classify.apply_features(extract_features, tweets)
classifier = nltk.NaiveBayesClassifier.train(training_set)

string = pickle.dumps(tweets_filtered)
fout = open('tweets_filtered.list', 'w')
fout.write(string)
fout.close()
